Honeycrisp
==========

Honeycrisp is the future of UI. It is a collection of advanced user interface controls and tools for developers to use in the next-generation of iOS apps.

With the introduction of iPhone, a whole new class of user interfaces were made possible thanks to the advent of touch screens. However, the touch screen was not the only thing that contributed to these revolutionary new user interfaces. These devices are capable of much more, and we can leverage them to further improve the design of user interfaces.

Honeycrisp takes advantage of the unique powers of next-generation computing devices. The user interface ideas of Honeycrisp can be categorized into three sections, or three connections.

---

### Context

**Connecting the UI with the environment outside the device.**

The user interface can take advantage of sensors available on these devices, like motion, and ambient lighting.

Using sensors of ambient light, the device can selectively increase contrast when necessary. The user perceives lower contrast of content on screen when outdoors than when indoors. The colors of specific important user interface elements can be shifted in such a way that contrast is increased. The user can now more comfortably use their device in more places.

The motion of the device can add physics-based animations of on-screen views. The relative positioning of the device to the user’s perspective can tell the user interface to parallax views that are on different visual planes or altitudes. This emphasizes the hierarchy of the user interface. For example, popovers not only just occlude content behind it, but parallax accordingly to the other on-screen content so that it appears visually “closer” to the user.

The motion of the device can suggest what kind of context the user is in. Is the user sitting quietly inside, or is the user riding their bike to work? The device can tailor it’s interface accordingly. If the device appears to be moving a lot, it suggests it’s not visible and likely in a pocket or purse. This also suggests it’s less likely to be noticed and heard. When the device needs to alert the user, the notification should be louder, and the vibration stronger. If the device appears motionless, it suggests it is lying flat on a table. Since the device is more likely to be visible and more likely to be heard, a notification does not need to be as loud. Often times, when lying flat on a table, a traditional vibration from a smartphone will cause a loud and obnoxious sound, since it’s resting against a hard surface, unlike a pocket or purse. Having alarms at an appropriate volume and amplitude helps make the experience feel more natural, and less like a dumb computer is yelling at you with every notification.
 
 
### Content

**Connecting the UI with the user's content.**

A user interface only exists to support an application’s content. What if the app’s UI could respond more dynamically to the content, instead of content being put in place of the UI? The content can dictate the UI instead of the other way around.

Typically, when designing a user interface, a designer will sketch out where all the UI belongs and put placeholders for all the content. However, not all content is the same. Content can be of different size and color and will work to varying degrees of success. For example, in a photo gallery application, a grid of square thumbnails of photos may be the UI. Every photograph will be cropped to be square. However, this part of often overlooked by designers and developers. When cropping a photo for a thumbnail, it shouldn’t just naively be cropped to the middle of the photo. The photo should be cropped so that the more important, and distinguishable features of the photo are within the cropped region. Honeycrisp allows you to automatically crop photos (to any size and aspect ratio) and maximize the number of important features in the photograph. No longer will you see thumbnails of your family photographs with cropped-out faces.


### Interactions

**Connecting the UI with user interaction for enhanced feedback.**

These next-generation devices have significantly more richer means of input than a traditional mouse and keyboard computer. It’s so much more intimate.

Unlike traditional, mouse-based computers, these devices take in much richer input. Taps don’t have to just be binary, like a mouse click. A traditional computer cannot tell how you clicked an element, but a multi-touch screen can. For instance, with some gestures, Honeycrisp can detect the handedness of the gesture. Now, the interface can adapt to the handedness of the gesture. When using a touch-based interface, the user’s hands will cover up a significant part of the display as it’s being used. Interfaces can be optimized and tailored, however slightly, depending on the handedness of the user.